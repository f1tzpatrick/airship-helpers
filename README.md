# Airship 2 Treasuremap Development Environment

This document describes how to deploy the airship-core `test-site` out of the Airship Treasuremap repository.

## **Part 1**: Virtual Machine Setup

### 1. Create a VM with:
  - 8 vCPU (eg 4 cores x 2 threads)
  - 24 GB RAM
  - 150 GB disk space
  - Nested Virtualization Enabled (Typically found in the settings where you configure CPUs for your VM)
  - Running [Ubuntu 20.04 server](https://ubuntu.com/download/server)

### 2. Create an SSH key pair to use with gerrit
  - Run `ssh-keygen`. Accept the default options, or set a password.
  - Run `cat ~/.ssh/id_rsa.pub` and add to your allowed SSH keys on gerrit, github


### 3. Clone the airshipctl and treasuremap repos
 > **Note:** This guide assumes they are cloned into your home directory , eg: `~/airshipctl`, `~/treasuremap`
  - `airshipctl`: https://review.opendev.org/admin/repos/airship/airshipctl
  - `treasuremap`: https://review.opendev.org/admin/repos/airship/treasuremap
  - `airship-helpers` (this repo; optional): https://github.com/f1tzpatrick/airship-helpers.git

### 4. Download test security key and add it to the environment

  - `~/airship-helpers/get-SOPS-key.sh`

```
curl -fsSL -o ~/dev-key.asc https://raw.githubusercontent.com/mozilla/sops/master/pgp/sops_functional_tests_key.asc
export SOPS_IMPORT_PGP="$(cat ~/dev-key.asc)"
export SOPS_PGP_FP="FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4"
```

## **Part 2**: Build the target cluster

The deployment scripts in treasuremap can be used to configure and build an airship-core `test-site`. The end result is a `target-cluster`, consisting of two nested virtual machines running kubernetes. This cluster is suitabe for developing and testing deployment documents on a single host machine.

### 0. Change the `target-cluster` Virtual Machine Specs (Optional)

- The VMs used for the target cluster will have these specs by default:
  - 2 vCPUs
  - 6-ish GB RAM

But these values can be changed in `treasuremap/playbooks/airship-treasuremap-build-gate.yaml` Find the section `airship_gate_flavors` and change these parameters:

```
# target_vm is the node running the control plane
target_vm_memory_mb: 7168
target_vm_vcpus: 2

# worker_vm is the node with the worker role
worker_vm_memory_mb: 7168
worker_vm_vcpus: 2
```

### 1. Gating Scripts

```
cd ~/treasuremap
./tools/gate/00_setup.sh
./tools/gate/10_build_gate.sh
./tools/gate/20_gate_runner.sh
```

**Note:** 20_run_gate_runner.sh should complete after 45 to 90 minutes.

### 2. Take ownership of the Environment

At this point airshipctl has created VMs and deployed kubernetes on top of them. There are some configuration updates we need to make in order to continue using this cluster.

- Add your user to the docker and libvirt groups if you want
- Save the secrets that were generated during deployment
- Take ownership of your airship config files and update your kubeconfig
- Set the kubeconfig context to target-cluster

The script `airship-helpers/take-ownership.sh` takes care of these tasks.

Afterwards, you should now be able to interact with your deployment:

- Confirm that the VMs are running:
  ```
  steven@airship-2:$ sudo virsh list
  Id    Name                           State
  ----------------------------------------------------
  6     air-target-1                   running
  10    air-worker-1                   running
  ```

- Confirm that kubectl is configured correctly:
  ```
  steven@airship-2:$ kubectl get nodes
  NAME     STATUS   ROLES    AGE   VERSION
  node01   Ready    master   39m   v1.18.6
  node03   Ready    worker   14m   v1.18.6
  ```

### 3. Configure the Airship config file to use your local treasuremap repository

As part of the deployment process, airshipctl generated a config file at `~/.airship/config`. We can update this file to improve parts of the development workflow.

The file `airship-helpers/treasuremap-config.yaml` contains the updated config file I use with my deployments. 

> **Note**: You'll need to update the paths in this file to line up with your home directory.

With those updates made, you can move on to step 4. For more detail about the airshipctl config file, read on!

The file is YAML format and has three main sections:

- `managementConfiguration`: We will keep the same values which were generated by airshipctl, however change the name from `dummy_management_config` to `treasuremap`
  ```
  managementConfiguration:
    treasuremap:
      insecure: true
      systemActionRetries: 30
      systemRebootDelay: 30
      type: redfish
  ```
- `contexts`: We will remove the ephemeral-cluster context, as we are only concerned with the target cluster from now on. We will update the `managementConfiguration` and set `manifest` to `treasuremap`.
  ```
  contexts:
    target-cluster:
      contextKubeconf: target-cluster_target
      managementConfiguration: treasuremap
      manifest: treasuremap
  ```
- `manifests`: We will configure this section to utilize the local copies of the repositories we cloned earlier in step 3.
  ```
  manifests:
    treasuremap:
      metadataPath: /manifests/site/test-site/metadata.yaml
      phaseRepositoryName: treasuremap
      targetPath: /home/<your username>
      repositories:
        airshipctl:
          url: /home/<your username>/airshipctl
        treasuremap:
          url: /home/<your username>/treasuremap
  ```

  **WARNING:** The use of `~` as the `targetPath` is for convenience. It allows a developer to work out of their home directory, and have their changes immediately available to `airshipctl`. When using local repositories like this, be careful about using `airshipctl document pull`, which could cause you to lose any working changes. `airshipctl document pull` will checkout the defined reference (default: `branch: master`). You can configure these references, but remember to always commit your changes before running `airshipctl document pull` when using `~` as your `targetPath`:
  ```
  repositories:
    treasuremap:
      url: /home/<your username>/treasuremap
      checkout:
        branch: my-dev-branch
        localBranch: true
  ```

The completed `~/.airship/config` file should look like this. It tells `airshipctl` to look to our local treasuremap repository for phase information, and to run those phases against the target-cluster.

```
apiVersion: airshipit.org/v1alpha1
kind: Config
currentContext: target-cluster
managementConfiguration:
  treasuremap:
    insecure: true
    systemActionRetries: 30
    systemRebootDelay: 30
    type: redfish
contexts:
  target-cluster:
    contextKubeconf: target-cluster_target
    managementConfiguration: treasuremap
    manifest: treasuremap
manifests:
  treasuremap:
    metadataPath: /manifests/site/test-site/metadata.yaml
    phaseRepositoryName: treasuremap
    targetPath: /home/steven
    repositories:
      airshipctl:
        url: /home/steven/airshipctl
      treasuremap:
        url: /home/steven/treasuremap
```
`airshipctl` is now configured to use any phase available on the `phase path` defined in `~/treasuremap/manifests/site/test-site/metadata.yaml`.

### 4. Validate `airshipctl` works correctly

We should now be able to view and run LMA phases

```
steven@airship:~/treasuremap$ ship phase list | grep workload
Phase/workload-target  target-cluster  KubernetesApply  target/workload
```

To further verify `airshipctl` is correctly configured, run `airshipctl phase render workload-target`. You will see the rendered yaml documents which would be applied by running the phase.

## Part 3: Developing Airship Manifests

With your `target-cluster` deployed and config file updated, you can now start working on manifests in your local `treasuremap` repository. Use `ship phase render` and `ship phase run` to test and apply manifest changes. Also, the script `~/treasuremap/tools/validate_docs` can be used to validate all manifests in the repo.

Treasuremap's `function` and `composite` and eventually `phase` manifests are used to compile various configurations into yaml documents that are fed to `kubectl apply`. Most `functions` start off simply, with a bare-bones flux `HelmRelease` document. Configurations are added at the `composite` level, then furthermore at the site `type` level (eg `airship-core` type, `multi-tenant` type, etc), and finally at the individual `site` manifest level. A `phase` can be thought of as an individual step in the deployment, and typically 'includes' multiple `composites` and `functions`. A `phase plan` can be used to define an entirely ordered deployment.

## Part 4: Final set up for LMA development

The LMA stack is assembled out of of several open source projects. These are distributed as helm charts, and we mange their deployments with flux `HelmRelease` documents. When tuning values structures, it can be handy to reference the chart source. The script `airship-helpers/charts.sh` will clone the github repositories of several of these projects. After cloning these repositories, you can use the VS Code Workspace file `airship-helpers/airship.code-workspace` to easily access the charts' source code.
